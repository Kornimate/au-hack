{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'path/to/annotations/instances_train2017.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m annotation_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath/to/annotations/instances_train2017.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      7\u001b[0m image_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath/to/train2017\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 9\u001b[0m coco \u001b[38;5;241m=\u001b[39m \u001b[43mCOCO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mannotation_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m target_classes \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m62\u001b[39m]  \u001b[38;5;66;03m# person and tv/monitor classes\u001b[39;00m\n\u001b[0;32m     12\u001b[0m cat_ids \u001b[38;5;241m=\u001b[39m coco\u001b[38;5;241m.\u001b[39mgetCatIds(catNms\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mperson\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtv\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pycocotools\\coco.py:81\u001b[0m, in \u001b[0;36mCOCO.__init__\u001b[1;34m(self, annotation_file)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloading annotations into memory...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     80\u001b[0m tic \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 81\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mannotation_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     82\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(dataset)\u001b[38;5;241m==\u001b[39m\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mannotation file format \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m not supported\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m(dataset))\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'path/to/annotations/instances_train2017.json'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pycocotools.coco import COCO\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "annotation_path = './annotations/instances_train2017.json'\n",
    "image_dir = 'path/to/train2017'\n",
    "\n",
    "coco = COCO(annotation_path)\n",
    "\n",
    "target_classes = [1, 62]  # person and tv/monitor classes\n",
    "cat_ids = coco.getCatIds(catNms=['person', 'tv'])\n",
    "\n",
    "img_ids = coco.getImgIds(catIds=cat_ids)\n",
    "images = coco.loadImgs(img_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_cv\n",
    "from tensorflow import keras\n",
    "\n",
    "# (including background as the 0th class)\n",
    "num_classes = 2  # person and tv/monitor\n",
    "retinanet = keras_cv.models.RetinaNet(\n",
    "    classes=num_classes,\n",
    "    backbone=\"resnet50\",\n",
    "    bounding_box_format=\"xywh\",\n",
    ")\n",
    "retinanet.compile(\n",
    "    classification_loss=\"focal\", \n",
    "    box_loss=\"smoothl1\", \n",
    "    optimizer=\"adam\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_and_boxes(img_id):\n",
    "    # Load image\n",
    "    img_info = coco.loadImgs(img_id)[0]\n",
    "    img_path = os.path.join(image_dir, img_info['file_name'])\n",
    "    image = tf.image.decode_jpeg(tf.io.read_file(img_path))\n",
    "    image = tf.image.resize(image, (640, 640))  # Resize for RetinaNet input\n",
    "\n",
    "    # Load annotations\n",
    "    ann_ids = coco.getAnnIds(imgIds=img_id, catIds=cat_ids, iscrowd=False)\n",
    "    annotations = coco.loadAnns(ann_ids)\n",
    "\n",
    "    boxes = []\n",
    "    labels = []\n",
    "    for ann in annotations:\n",
    "        bbox = ann['bbox']\n",
    "        category_id = ann['category_id']\n",
    "        label = target_classes.index(category_id) + 1  # Offset for background class\n",
    "\n",
    "        boxes.append(bbox)\n",
    "        labels.append(label)\n",
    "    \n",
    "    boxes = np.array(boxes, dtype=np.float32)\n",
    "    labels = np.array(labels, dtype=np.int32)\n",
    "    \n",
    "    return image, {\"boxes\": boxes, \"classes\": labels}\n",
    "\n",
    "def coco_generator(img_ids):\n",
    "    for img_id in img_ids:\n",
    "        yield load_image_and_boxes(img_id)\n",
    "\n",
    "# Create TensorFlow dataset\n",
    "dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: coco_generator(img_ids),\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(640, 640, 3), dtype=tf.float32),\n",
    "        {\n",
    "            \"boxes\": tf.TensorSpec(shape=(None, 4), dtype=tf.float32),\n",
    "            \"classes\": tf.TensorSpec(shape=(None,), dtype=tf.int32),\n",
    "        },\n",
    "    ),\n",
    ")\n",
    "dataset = dataset.batch(4).prefetch(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retinanet.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: coco_generator(img_ids),\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(640, 640, 3), dtype=tf.float32),\n",
    "        {\n",
    "            \"boxes\": tf.TensorSpec(shape=(None, 4), dtype=tf.float32),\n",
    "            \"classes\": tf.TensorSpec(shape=(None,), dtype=tf.int32),\n",
    "        },\n",
    "    ),\n",
    ")  # validation data\n",
    "retinanet.evaluate(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(image_path):\n",
    "    image = tf.image.decode_jpeg(tf.io.read_file(image_path))\n",
    "    image = tf.image.resize(image, (640, 640))\n",
    "    image = tf.expand_dims(image, axis=0)\n",
    "\n",
    "    # Predict bounding boxes and classes\n",
    "    predictions = retinanet.predict(image)\n",
    "    return predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
